{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWmRfQQyiRTQNB3dbA6TXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaketMunda/introduction-to-nlp/blob/master/nlp_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing with TensorFlow\n",
        "\n",
        "NLP has the goal of deriving information out of natural language (could be sequences text or speech).\n",
        "\n",
        "Another common term for NLP problems is sequence to sequence problems (seq2seq)."
      ],
      "metadata": {
        "id": "Ep05V4vWV7mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we're going to experiment deep-learning models so we need to enable GPUs\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubzX8Z9hWWcu",
        "outputId": "0f6b4f40-56a0-4a0f-ba6c-a0880eb60b41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-1778e88f-9b17-69ee-6953-4a35097deca6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Helper functions"
      ],
      "metadata": {
        "id": "Tyusp2doWbLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get helper_functions.py script from Github\n",
        "!wget https://raw.githubusercontent.com/SaketMunda/ml-helpers/master/helper_functions.py\n",
        "\n",
        "from helper_functions import unzip_data, create_tensorboard_callback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlTigivCWskW",
        "outputId": "6fe52b67-d884-4800-8d73-c1be77717bea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 04:53:41--  https://raw.githubusercontent.com/SaketMunda/ml-helpers/master/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2904 (2.8K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]   2.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-07 04:53:42 (59.4 MB/s) - ‘helper_functions.py’ saved [2904/2904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get a Text Dataset\n",
        "The dataset that we're going to be using is Kaggle's introduction to NLP dataset (text samples of Tweets labelled as disaster or not disaster)\n",
        "\n",
        "See the original source here: https://www.kaggle.com/c/nlp-getting-started"
      ],
      "metadata": {
        "id": "CZ_5BTF-W7K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
        "\n",
        "# unzip the data\n",
        "unzip_data('nlp_getting_started.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F88ynPezXO3A",
        "outputId": "f1797c14-2412-44fc-887d-19a005cbde3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 04:53:44--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.111.128, 142.251.16.128, 142.251.163.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.111.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "\rnlp_getting_started   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2023-02-07 04:53:44 (109 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Text Dataset\n",
        "\n",
        "To visualize our text samples, we first have to read them in, so we can do it through pandas."
      ],
      "metadata": {
        "id": "WyMBzit1XakQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# check the shapes\n",
        "train_df.shape, test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwqhQM8yXl7f",
        "outputId": "86d7b38d-71f5-41e4-8b11-99f0d9420edb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7613, 5), (3263, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view some samples\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HnvCJoJRXvcI",
        "outputId": "ed4e7e67-28ef-478b-9579-8d0b2073a7a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c809d819-9463-4c90-a311-32f152589c0f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c809d819-9463-4c90-a311-32f152589c0f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c809d819-9463-4c90-a311-32f152589c0f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c809d819-9463-4c90-a311-32f152589c0f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here, `text` is the tweet and `target` variable is to identify whether the tweet is a disaster or not, so if `1` then it's a disaster else not a disaster.\n",
        "\n",
        "Let's visualize some random `training` samples, but before that this is a good practice to shuffle the training samples first,"
      ],
      "metadata": {
        "id": "A1v_p_ElXytS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_shuffled = train_df.sample(frac=1, random_state=17)\n",
        "# frac=1 means 100% of samples will be shuffled\n",
        "train_df_shuffled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "p17uRynCYEDW",
        "outputId": "a085ba76-84df-4811-fa0a-81ff9a2bf2c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id            keyword                    location  \\\n",
              "7027  10072            typhoon                         NaN   \n",
              "318     463         armageddon                         NaN   \n",
              "1681   2425            collide  www.youtube.com?Malkavius2   \n",
              "5131   7318  nuclear%20reactor          New York, New York   \n",
              "2967   4262           drowning          Hendersonville, NC   \n",
              "...     ...                ...                         ...   \n",
              "406     584              arson           Jerusalem, Israel   \n",
              "5510   7863        quarantined                 Livonia, MI   \n",
              "2191   3139             debris                         NaN   \n",
              "7409  10600            wounded               santo domingo   \n",
              "2671   3833           detonate    back in japan ??????????   \n",
              "\n",
              "                                                   text  target  \n",
              "7027  Typhoon Soudelor: When will it hit Taiwan ÛÒ ...       1  \n",
              "318   RT @RTRRTcoach: #Love #TrueLove #romance lith ...       0  \n",
              "1681  I liked a @YouTube video from @gassymexican ht...       0  \n",
              "5131  Japan's Restart of Nuclear Reactor Fleet Fast ...       1  \n",
              "2967  #ICYMI #Annoucement from Al Jackson... http://...       0  \n",
              "...                                                 ...     ...  \n",
              "406   Mourning notices for stabbing arson victims st...       1  \n",
              "5510  Reddit's new content policy goes into effect m...       0  \n",
              "2191  Plane debris discovered on Reunion Island belo...       1  \n",
              "7409  Police Officer Wounded Suspect Dead After Exch...       1  \n",
              "2671  Detonate (feat. M?.?O?.?P?.?)\\nfrom Grandeur b...       0  \n",
              "\n",
              "[7613 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0778b09-fadc-448d-97c6-ba169775e069\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7027</th>\n",
              "      <td>10072</td>\n",
              "      <td>typhoon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor: When will it hit Taiwan ÛÒ ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>463</td>\n",
              "      <td>armageddon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @RTRRTcoach: #Love #TrueLove #romance lith ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1681</th>\n",
              "      <td>2425</td>\n",
              "      <td>collide</td>\n",
              "      <td>www.youtube.com?Malkavius2</td>\n",
              "      <td>I liked a @YouTube video from @gassymexican ht...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5131</th>\n",
              "      <td>7318</td>\n",
              "      <td>nuclear%20reactor</td>\n",
              "      <td>New York, New York</td>\n",
              "      <td>Japan's Restart of Nuclear Reactor Fleet Fast ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2967</th>\n",
              "      <td>4262</td>\n",
              "      <td>drowning</td>\n",
              "      <td>Hendersonville, NC</td>\n",
              "      <td>#ICYMI #Annoucement from Al Jackson... http://...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>584</td>\n",
              "      <td>arson</td>\n",
              "      <td>Jerusalem, Israel</td>\n",
              "      <td>Mourning notices for stabbing arson victims st...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5510</th>\n",
              "      <td>7863</td>\n",
              "      <td>quarantined</td>\n",
              "      <td>Livonia, MI</td>\n",
              "      <td>Reddit's new content policy goes into effect m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2191</th>\n",
              "      <td>3139</td>\n",
              "      <td>debris</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Plane debris discovered on Reunion Island belo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7409</th>\n",
              "      <td>10600</td>\n",
              "      <td>wounded</td>\n",
              "      <td>santo domingo</td>\n",
              "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2671</th>\n",
              "      <td>3833</td>\n",
              "      <td>detonate</td>\n",
              "      <td>back in japan ??????????</td>\n",
              "      <td>Detonate (feat. M?.?O?.?P?.?)\\nfrom Grandeur b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0778b09-fadc-448d-97c6-ba169775e069')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0778b09-fadc-448d-97c6-ba169775e069 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0778b09-fadc-448d-97c6-ba169775e069');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how does the test set looks like ?\n",
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "P0I_-hG1YYwu",
        "outputId": "6c046d4c-bc63-403b-bf17-85aeb4978f62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aee74988-b4ef-4572-b82c-6adcee276bd1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aee74988-b4ef-4572-b82c-6adcee276bd1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aee74988-b4ef-4572-b82c-6adcee276bd1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aee74988-b4ef-4572-b82c-6adcee276bd1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many examples of each class ?\n",
        "train_df['target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9FJ8OerYhVh",
        "outputId": "cc1fbbf8-0b48-4fd2-c57e-ca9ac0ddaa7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize some random samples\n",
        "\n",
        "import random\n",
        "random_index = random.randint(0, len(train_df_shuffled)-5)\n",
        "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"Target: {target}\", \"(disaster)\" if target > 0 else \"(not a disaster)\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"----------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgQTdLVzYtcu",
        "outputId": "b6604176-2494-4b47-ca21-95612b7a9102"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: 0 (not a disaster)\n",
            "Text:\n",
            "@Caitsroberts see U the night wee bArra to get absolutely wrecked ????\n",
            "\n",
            "----------\n",
            "\n",
            "Target: 0 (not a disaster)\n",
            "Text:\n",
            "I barely smoke with people i solo all my blunts\n",
            "\n",
            "----------\n",
            "\n",
            "Target: 0 (not a disaster)\n",
            "Text:\n",
            "Listening to Blowers and Tuffers on the Aussie batting collapse at Trent Bridge reminds me why I love @bbctms! Wonderful stuff! #ENGvAUS\n",
            "\n",
            "----------\n",
            "\n",
            "Target: 1 (disaster)\n",
            "Text:\n",
            "Tonight we have attended a fire in Romford with @LondonFire thankfully no injuries http://t.co/iyjeJop2WI\n",
            "\n",
            "----------\n",
            "\n",
            "Target: 0 (not a disaster)\n",
            "Text:\n",
            "http://t.co/EQjCpWILVn: Articles In Saudi Press Reject Russian Initiative For Regional Alliance With Assad Regime To Fight Terrorism\n",
            "\n",
            "----------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset into Train and Validation sets\n",
        "\n",
        "Since the test set doesn't contain the target variable so we might need some unseen data for model to be validated after training, so how about splitting our training set for validating purpose with some amount.\n"
      ],
      "metadata": {
        "id": "WmmI2TzZZa1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.1,\n",
        "                                                                            random_state=17)"
      ],
      "metadata": {
        "id": "jEgDccMKTYSx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Text into Numbers\n",
        "\n",
        "Our labels are in numerical form (0 and 1) but our tweets are in string form.\n",
        "\n",
        "But machine learning algorithm learns only through numbers so we have to convert those tweets/texts into numbers.\n",
        "\n",
        "In NLP, there are two main concepts for turning text into numbers,\n",
        "- **Tokenization** : A straight mapping from **word**(known as *word-level tokenization*) or character(which is *character-level tokenization*) or sub-word(*sub-word tokenization*) to a numerical value. Just like One hot encoding, suppose we have a sentence as \"My name is Alpha\", then if we are mapping according to word, \"My\" would `0`, \"name\" as `1`, \"is\" as `2` and \"Alpha\" as `3`.\n",
        "- **Embeddings** : An embedding is a representation of natural language which can be learned. Representation comes in the form of **feature-vector**. For example the word \"Alpha\" could be represented by 5-D vector `[0.564, 0.897, 0.456, -0.987, 0.15]`. The size of the feature vector is tuneable. There are two ways to use embeddings:\n",
        "\n",
        "    - **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as `tf.keras.layers.Embedding`) and an embedding representation will be learned during model training.\n",
        "    - **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.\n",
        "\n",
        "\n",
        "Simply, \n",
        "\n",
        "**Tokenization** : Straight mapping from word to number.\n",
        "\n",
        "**Embedding** : Richer representation of relationships between tokens.\n",
        "\n",
        "It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using [tf.keras.layers.concatenate](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)).\n",
        "\n",
        "If you're looking for pre-trained word embeddings, [Word2vec embeddings](https://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on TensorFlow Hub are great places to start.\n",
        "\n",
        "Much like searching for a pre-trained computer vision model, we can search for pre-trained word embedding to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\"."
      ],
      "metadata": {
        "id": "MitNUg-uTxx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Vectorization\n",
        "\n",
        "Mapping words to numbers.\n",
        "\n",
        "To tokenize our words, we'll use the preprocessing layer,\n",
        "`tf.keras.layers.preprocessing.TextVectorization`"
      ],
      "metadata": {
        "id": "oyTzuPa0V5bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Using the default TextVectorization variables\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                 standardize=\"lower_and_strip_punctuation\", # how to process the text\n",
        "                                 split=\"whitespace\", # how to split the text\n",
        "                                 ngrams=None, # create groups of n-words\n",
        "                                 output_mode='int', # how to map tokens to numbers\n",
        "                                 output_sequence_length=None) # How long should the output sequence of tokens be?"
      ],
      "metadata": {
        "id": "k6z0y1iEIEs5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "About the above params,\n",
        "\n",
        "- `max_tokens` : The maximum number of words in your vocabulary (e.g 20000 or the number of unique words in your text), includes a value for OOV(out of vocabulary) tokens\n",
        "- `standardize` : Methods for standardizing text\n",
        "- `split`: split the text\n",
        "- `ngrams`: how many words to contain per token split, for example if 2, it splits tokens into continous sequences of 2\n",
        "- `output_mode`: How to output tokens can be `int`(integer mapping), `binary`(OHE), `count` or `tf-idf`\n",
        "- `output_sequence_length`: Length of tokenized sequence to output, For example if set to 150, all tokenized sequences will be 150 tokens long.\n",
        "\n",
        "In the above cell, we have initialized the object with the default settings but let's customize it a little bit for our own use case.\n",
        "\n",
        "In particular, let's set values for `max_tokens` and `output_sequence_length`.\n",
        "\n",
        "For `max_tokens`(the number of words in the vocabulary), multiples of 10,000(`10,000`, `20,000`, `30,000`) or the exact number of unqiue words in your text(e.g `32,179`) are common values.\n",
        "\n",
        "For our use case, `10,000`\n",
        "\n",
        "And for the `output_sequence_length` we'll use the average number of tokens per Tweet in the training set. But first, we'll need to find it."
      ],
      "metadata": {
        "id": "CgAnhcS2KVIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find average number of tokens (words) in training tweets\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "metadata": {
        "id": "c4ysmlUhLX6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3dd4425-3dd0-4dc0-cd48-9e8ca23f6b04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup text vectorization with custom variables\n",
        "max_vocab_length = 10000\n",
        "max_length = 15\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode='int',\n",
        "                                    output_sequence_length=max_length)"
      ],
      "metadata": {
        "id": "KA553WXeaU4a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To map our `TextVectorization` instance `text_vectorizer` to our data, we can call the `adapt()` method on it whilst passing it our training set."
      ],
      "metadata": {
        "id": "KJZbqKiobF3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "OOfO1BecbdMe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data mapped! Let's try our `text_vectorizer` on a custom sentence."
      ],
      "metadata": {
        "id": "h0hxXVglbj3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample sentence\n",
        "sample_sentence = \"There's a flood in my village!\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGCO1vkUb3eW",
        "outputId": "568386e4-91db-4823-e7b7-979cc0515f3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[281,   3, 214,   4,  13, 881,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try our `text_vectorizer` on a few random sentences ?"
      ],
      "metadata": {
        "id": "LC1QnE2WcBfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f'Original Text: \\n{random_sentence}\\\n",
        "        \\n\\n Vectorized Text:')\n",
        "text_vectorizer([random_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYj-_VcNcP7u",
        "outputId": "739dd278-9c36-4390-f438-fcb3e8788540"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: \n",
            "Bayelsa poll: Tension in Bayelsa as Patience Jonathan plans to hijack APC PDP: Plans by former First Lady and... http://t.co/3eJL9lZlCH        \n",
            "\n",
            " Vectorized Text:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[1023, 2039, 1467,    4, 1023,   26,  944, 1119,  618,    5,  652,\n",
              "        1329, 1501,  618,   18]])>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also check the unique tokens in our vocabulary using the `get_vocabulary()` method"
      ],
      "metadata": {
        "id": "ZrtMKcpgcfcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
        "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
        "\n",
        "print(f\"Number of words in Vocab:{len(words_in_vocab)}\")\n",
        "print(f\"Top 5 most common words:{top_5_words}\")\n",
        "print(f\"Bottom 5 least common words:{bottom_5_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJLbch5kcuc5",
        "outputId": "3d72d379-2403-4887-97e1-2280c3bd36fc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in Vocab:10000\n",
            "Top 5 most common words:['', '[UNK]', 'the', 'a', 'in']\n",
            "Bottom 5 least common words:['ovo', 'overåÊhostages', 'overzero', 'overwatch', 'overturns']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding\n",
        "\n",
        "**Create an Embedding using Embedding Layer**\n",
        "\n",
        "We've got a way to map our text to numbers. How about we go a step further and turn those numbers into an embedding?\n",
        "\n",
        "The powerful thing about an embedding is it can be learned during training. This means rather that just be static, a word's numeric representation can be improved as a model goes through data samples.\n",
        "\n",
        "We can see what an embedding of a word looks like by using the `tf.keras.layers.Embedding` layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "rnhKQAUFdRqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(17)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                             output_dim=128,\n",
        "                             embeddings_initializer=\"uniform\",\n",
        "                             input_length=max_length,\n",
        "                             name=\"embedding_1\")\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4AxkJ4teKHO",
        "outputId": "92d95a6b-d153-499c-a919-17f4cfec51fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.embedding.Embedding at 0x7f0c003b87c0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`embedding` is a TensorFlow Layer, so that we can use it as part of a model, meaning its parameters(word representations) can be updated and improved as the model learns.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wd2p3RPvevde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random sentence from training set\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Original text: \\n{random_sentence}\")\n",
        "print(\"\\n\\nEmbedded version:\")\n",
        "\n",
        "# embed the random sentence\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GseLqO74tWkN",
        "outputId": "8be5c03a-cdd8-40ea-ad2a-134a5a67ee37"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: \n",
            "WPRI 12 Eyewitness News Rhode Island set to modernize its voting equipment WPRI 12 EyewitnessÛ_ http://t.co/aP9JBrPmQg\n",
            "\n",
            "\n",
            "Embedded version:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[-0.01085513, -0.04794589, -0.00775063, ..., -0.00499675,\n",
              "         -0.0077184 , -0.04034487],\n",
              "        [ 0.03000946,  0.01517535,  0.0357525 , ..., -0.0011016 ,\n",
              "          0.02560176,  0.01277179],\n",
              "        [-0.021665  , -0.03029667, -0.03140029, ..., -0.03586438,\n",
              "         -0.00279536, -0.0279383 ],\n",
              "        ...,\n",
              "        [-0.01085513, -0.04794589, -0.00775063, ..., -0.00499675,\n",
              "         -0.0077184 , -0.04034487],\n",
              "        [ 0.03000946,  0.01517535,  0.0357525 , ..., -0.0011016 ,\n",
              "          0.02560176,  0.01277179],\n",
              "        [-0.02031898,  0.03425609,  0.03940277, ..., -0.04002044,\n",
              "          0.01343619, -0.00712932]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary.\n",
        "\n",
        "If we review the shape of Embedded Version Tensor it is, `(1, 15, 128)`, it means that,\n",
        "- 1: Is the quantity of sequences(sentences) we passed\n",
        "- 15: is the `max_length` that we decided to normalize every sentence, if greater than 15 tokens/words then trim extra tokens or if less than 15 then pad it.\n",
        "- 128: is the array size of each words, for example above sentence as `Now`, for this token the embedding tensor will look like,"
      ],
      "metadata": {
        "id": "9Kp2cu1Dt4sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_embed[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW7O7e1UDyKn",
        "outputId": "835994a1-ffa0-4dab-cd28-1af44e76c587"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              "array([-1.08551271e-02, -4.79458943e-02, -7.75063038e-03, -4.22770269e-02,\n",
              "        2.88492478e-02, -3.78711820e-02, -3.77515927e-02, -4.94191647e-02,\n",
              "       -9.12709162e-03,  2.85468139e-02,  9.33952257e-03,  2.45751403e-02,\n",
              "       -3.05716041e-02, -1.92692876e-02,  1.29480697e-02, -3.71055491e-02,\n",
              "        1.10397823e-02, -2.67873891e-02, -3.27140689e-02, -1.73388608e-02,\n",
              "       -2.82084942e-03,  3.78791727e-02, -2.82996185e-02, -6.49726391e-03,\n",
              "        4.11892794e-02,  4.22844402e-02,  6.12200424e-03,  3.62223387e-03,\n",
              "        2.85006277e-02, -4.20019515e-02, -2.99638510e-03,  2.85754241e-02,\n",
              "        1.57457590e-03, -2.51387842e-02,  1.01218335e-02, -4.06199470e-02,\n",
              "        9.52048227e-03, -3.57661732e-02, -3.01040057e-02, -7.92275742e-03,\n",
              "        4.36102413e-02,  4.63193394e-02, -4.67063673e-02, -2.75242459e-02,\n",
              "       -4.16955128e-02,  5.17893583e-04, -4.97056842e-02, -1.39452145e-03,\n",
              "        4.49001230e-02,  1.94833614e-02,  3.98296006e-02, -1.86043009e-02,\n",
              "       -4.24662344e-02, -2.55108010e-02,  2.29851492e-02, -2.89917234e-02,\n",
              "       -1.73102990e-02, -8.22129101e-03,  3.67656462e-02,  5.13192266e-03,\n",
              "        4.75645065e-04, -5.85453585e-03,  1.55254491e-02, -4.01584134e-02,\n",
              "       -2.33743545e-02,  1.50512494e-02, -4.47244756e-02,  3.38714756e-02,\n",
              "        7.53736496e-03, -3.43751162e-04,  4.12539393e-03,  4.98585775e-03,\n",
              "       -1.65010467e-02, -1.27005465e-02, -2.88642570e-03,  1.28503703e-02,\n",
              "        3.89880650e-02,  2.49091424e-02,  3.56270932e-02,  3.93487513e-04,\n",
              "       -3.99441496e-02, -6.02000952e-03,  1.66440941e-02, -4.76603769e-02,\n",
              "       -8.25151056e-03, -4.32131551e-02,  4.86583747e-02,  4.97114658e-03,\n",
              "       -6.62803650e-05,  2.97801532e-02,  4.79086898e-02,  7.71423429e-03,\n",
              "        1.93127245e-03, -1.74502283e-03, -3.70806344e-02,  4.04000990e-02,\n",
              "       -2.80012935e-03, -4.36211340e-02,  4.89921086e-02, -3.71468551e-02,\n",
              "       -4.01755571e-02,  4.50927950e-02,  3.49009670e-02, -1.28290281e-02,\n",
              "        3.18916924e-02,  3.37137617e-02,  2.35194899e-02, -9.96991247e-03,\n",
              "        3.28006484e-02, -4.33552861e-02,  1.60355605e-02,  1.04300380e-02,\n",
              "       -1.00656152e-02,  4.30138968e-02,  3.21648754e-02,  3.58772278e-03,\n",
              "       -2.11923718e-02,  4.36104648e-02, -2.93122530e-02, -2.66527068e-02,\n",
              "        3.86171378e-02, -1.84558518e-02, -4.67791930e-02, -3.13358456e-02,\n",
              "        3.29025649e-02, -4.99675423e-03, -7.71839544e-03, -4.03448716e-02],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation function for our Model Experiments\n",
        "\n",
        "Since we're going to perform multiple experiments by creating deeplearning models and scikit learn algorithms, so to track them we should create a common function for comparison.\n",
        "\n",
        "There are various metrics to evaluate the classification models like precision, f1-score, recall. So let's look at them through a function altogether."
      ],
      "metadata": {
        "id": "dbKbKEU5Fxy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates Model accuracy, precision, recall and f1-score for a binary classification model\n",
        "  \"\"\"\n",
        "  # calculate the model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred)\n",
        "  # calculate precision, recall, f1-score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy * 100,\n",
        "                   \"precision\":model_precision,\n",
        "                   \"recall\": model_recall,\n",
        "                   \"f1-score\": model_f1}\n",
        "                  \n",
        "  return model_results"
      ],
      "metadata": {
        "id": "bcwg-4J5GKUj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling a Text Dataset\n",
        "\n",
        "For experiments with various machine learning model for text classifier we will be considering below experiments:\n",
        "- **Model 0** : Naive Bayes (baseline)\n",
        "- **Model 1** : Feed-forward neural network (dense model) \n",
        "- **Model 2** : LSTM Model (RNN)\n",
        "- **Model 3** : GRU (RNN)\n",
        "- **Model 4** : Bidirectional-LSTM (RNN)\n",
        "- **Model 5** : 1D CNN\n",
        "- **Model 6** : TF Hub Pre-trained Feature Extractor\n",
        "- **Model 7** : Same as model 6 with 10% of training samples"
      ],
      "metadata": {
        "id": "LUy8YjJ2EWQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 0 : Getting a baseline\n",
        "\n",
        "We'll use `scikit-learn` library for building this model, and create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert words into numbers and then model them using Multinomial Naive Bayes Algorithm.\n",
        "\n",
        "> 📖 **Reading**: About TF-IDF on [Scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)"
      ],
      "metadata": {
        "id": "72x7n-xuFM8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IkZ6K-UF1j2",
        "outputId": "99cad331-c082-44f0-c6db-c0a28798a68e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial Naive Bayes model is kind of shallow model which trains faster."
      ],
      "metadata": {
        "id": "gLex6UKjGaDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zihkmgPZGkfC",
        "outputId": "a5f94e95-dc50-41a1-c425-49a54534d428"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our baseline model achieves an accuracy of: 80.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make some predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZtZ-lodG0ii",
        "outputId": "0a136917-7f8b-4544-dbe1-4f6647d14897"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 1, 0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it is similar to our labels\n",
        "val_labels[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uWMFYvAG-qx",
        "outputId": "331b5630-d7ce-43e0-986c-c02cabd36fb4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the results\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbG9m6vqJhGL",
        "outputId": "74dbd6b9-29b2-4862-944e-4e1c9cd16023"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 80.18372703412074,\n",
              " 'precision': 0.8125567744156732,\n",
              " 'recall': 0.8018372703412073,\n",
              " 'f1-score': 0.7968681002825004}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 : A Simple Dense Model\n",
        "\n",
        "The first \"deep\" model we're going to build is a single layer dense model."
      ],
      "metadata": {
        "id": "qZJjPR31JqYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create directory to save Tensorboard logs\n",
        "SAVE_DIR = \"model_logs\"\n",
        "\n",
        "# Build model with functional API\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string) # inputs are 1-dimensional strings\n",
        "x = text_vectorizer(inputs) # turn text inputs into numbers using text_vectorizer\n",
        "x = embedding(x) # create an embedding of the numerized numbers\n",
        "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model\n",
        "\n",
        "# compile the model\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# fit the model\n",
        "history_model_1 = model_1.fit(x=train_sentences,\n",
        "                              y=train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
        "                                                                     experiment_name=\"simple_dense_model\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh4r0AfCHfnh",
        "outputId": "71327a26-8614-4ee3-a852-0ad3a0184894"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tensorboard log files to: model_logs/simple_dense_model/20230207-045350\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 4s 5ms/step - loss: 0.6152 - accuracy: 0.6854 - val_loss: 0.5256 - val_accuracy: 0.7638\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.4439 - accuracy: 0.8167 - val_loss: 0.4640 - val_accuracy: 0.7874\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 1s 5ms/step - loss: 0.3478 - accuracy: 0.8616 - val_loss: 0.4596 - val_accuracy: 0.7927\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 8ms/step - loss: 0.2846 - accuracy: 0.8864 - val_loss: 0.4721 - val_accuracy: 0.7979\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.2380 - accuracy: 0.9101 - val_loss: 0.4938 - val_accuracy: 0.7953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixLdaS-DJiv7",
        "outputId": "d39f02a0-d64c-4484-816d-444282c7f8f9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4938 - accuracy: 0.7953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4938335120677948, 0.7952755689620972]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make some predictions\n",
        "model_1_pred_probs = model_1.predict(val_sentences)\n",
        "model_1_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFZ35FbBJrW-",
        "outputId": "c4a650a8-8001-46ab-a411-98de2bc8c2bd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12096813],\n",
              "       [0.00963378],\n",
              "       [0.07517964],\n",
              "       [0.43156   ],\n",
              "       [0.9998043 ],\n",
              "       [0.8580827 ],\n",
              "       [0.13515034],\n",
              "       [0.894733  ],\n",
              "       [0.58565694],\n",
              "       [0.08657645]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright ! Let's some more evaluations by using our common function for evaluation."
      ],
      "metadata": {
        "id": "SrsxhgWcKgH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for the evaluation we have to make it similar to our val_labels which is in 0 and 1\n",
        "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
        "model_1_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "191_2ce5K5hV",
        "outputId": "f55d2d18-2a0c-49df-bee6-cf2fb3c2a066"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 1., 0., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_dense_results = calculate_results(y_true=val_labels,\n",
        "                                         y_pred=model_1_preds)\n",
        "\n",
        "simple_dense_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_qe3BuhKpa8",
        "outputId": "1d1644b4-7a04-4243-99b6-94df4a63371b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.52755905511812,\n",
              " 'precision': 0.7966788703003602,\n",
              " 'recall': 0.7952755905511811,\n",
              " 'f1-score': 0.7932321923411083}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For comparing the results like these between two models, let's create a helper function"
      ],
      "metadata": {
        "id": "WBvkM6hNLKma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
        "  for k,v in baseline_results.items():\n",
        "    print(f\"Baseline {k}: {v:.2f}, New {k}: {new_model_results[k]:.2f}, Difference: {new_model_results[k] - v:.3f}\")"
      ],
      "metadata": {
        "id": "-lpajBzWLZ2b"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_baseline_to_new_results(baseline_results, simple_dense_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usjwQEZSMA9v",
        "outputId": "6022774f-753d-4806-a861-b9ba9948a036"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 80.18, New accuracy: 79.53, Difference: -0.656\n",
            "Baseline precision: 0.81, New precision: 0.80, Difference: -0.016\n",
            "Baseline recall: 0.80, New recall: 0.80, Difference: -0.007\n",
            "Baseline f1-score: 0.80, New f1-score: 0.79, Difference: -0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Learned Embeddings\n",
        "\n",
        "Our first model (`model_1`) contained an embedding layer (`embedding`) which learned a way of representing words as feature vectors by passing over the training data.\n",
        "\n",
        "To understand what a text embedding is, let's visualize the embedding our model learned."
      ],
      "metadata": {
        "id": "Hysy96wpMGLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vocabulary from the text vectorization layer\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "len(words_in_vocab), words_in_vocab[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp7kot3HIkVN",
        "outputId": "28ea22c9-f7b0-491f-bbdb-f7ecb673eeac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's get our embedding layer's weights (these are the numerical representations of each word)."
      ],
      "metadata": {
        "id": "ObaHI9rrIujy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc39TunRJDn-",
        "outputId": "9c318fd8-6dd7-4616-a0f8-b52a7a3e7bee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 128)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the weight matrix of embedding layer\n",
        "embed_weights = model_1.layers[2].get_weights()[0] \n",
        "print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cnOYapAJFQ-",
        "outputId": "279d439e-af2c-4073-d184-a8228123555f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_weights[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JDpi0keJaKg",
        "outputId": "7c2f59e1-36da-4e1a-caa1-c0f8df4cd61a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.05793181, -0.03914463, -0.02473384, -0.00400929,  0.02946341,\n",
              "       -0.01301206,  0.05229731,  0.04887475,  0.0602509 ,  0.00805069,\n",
              "        0.06344495,  0.06382315,  0.03528398, -0.02437459, -0.0378452 ,\n",
              "       -0.00860966, -0.02997298, -0.00939368,  0.05955039, -0.06665283,\n",
              "       -0.01535041,  0.01206797,  0.0132463 ,  0.00885766, -0.03989454,\n",
              "        0.0059811 , -0.05867314,  0.01391947, -0.01342948, -0.03151252,\n",
              "       -0.01170612, -0.01470304,  0.0555351 ,  0.06478361, -0.03305589,\n",
              "        0.00180234,  0.03188965, -0.01603715,  0.0065343 , -0.0136434 ,\n",
              "       -0.01092491,  0.03955517, -0.00912936, -0.04180073, -0.01730738,\n",
              "       -0.00267581, -0.05257863, -0.00593809, -0.00243137, -0.00406674,\n",
              "        0.05934991,  0.0077191 , -0.00551353, -0.0347125 , -0.02665271,\n",
              "       -0.01058053,  0.01899304, -0.0502522 , -0.01186904, -0.00643934,\n",
              "       -0.0397414 , -0.02110855, -0.00443364, -0.0067405 ,  0.02272329,\n",
              "        0.02054001, -0.01021376, -0.03714589,  0.03495809, -0.00551941,\n",
              "       -0.05402943,  0.05673279, -0.02860853,  0.05933988,  0.04321827,\n",
              "       -0.03300177, -0.01398474,  0.00378326,  0.0565896 ,  0.06639098,\n",
              "        0.04891377, -0.03926005,  0.02138171, -0.05530741,  0.03143611,\n",
              "       -0.00662654,  0.02964565, -0.05007802, -0.04113483,  0.01724831,\n",
              "        0.00210688,  0.01717916,  0.02345618,  0.04754276, -0.02072727,\n",
              "        0.02285102,  0.03893686,  0.03659019,  0.03092393, -0.05657145,\n",
              "       -0.00815741, -0.03492344, -0.05708261, -0.01064817, -0.04797028,\n",
              "       -0.01948298, -0.0337585 , -0.01503072, -0.02164165,  0.00636747,\n",
              "        0.03055428,  0.06135831, -0.01142473, -0.02363287,  0.01905317,\n",
              "       -0.01181161, -0.03480681, -0.03662146,  0.03230286, -0.05484902,\n",
              "       -0.01756295,  0.03628292,  0.02103203,  0.03848095,  0.05498388,\n",
              "        0.00409453,  0.0232612 , -0.00015125], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got these two objects, we can use the [Embedding Projector Tool](http://projector.tensorflow.org/) to visualize our embedding.\n",
        "\n",
        "To use the embedding projector tool, we need two files,\n",
        "- the embedding vectors (same as embedding weights)\n",
        "- the metadata of the embedding vectors (the words they represent - our vocabulary)"
      ],
      "metadata": {
        "id": "v8q7clW_JvQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import io\n",
        "\n",
        "# # Create output writers\n",
        "# out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
        "# out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "# # Write embedding vectors and words to file\n",
        "# for num, word in enumerate(words_in_vocab):\n",
        "#   if num==0:\n",
        "#     continue # skip padding token\n",
        "#   vec = embed_weights[num]\n",
        "#   out_m.write(word + \"\\n\") # write words to file\n",
        "#   out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
        "\n",
        "# out_v.close()\n",
        "# out_m.close()\n",
        "\n",
        "\n",
        "# # Download files locally to upload to Embedding projector\n",
        "\n",
        "# try:\n",
        "#   from google.colab import files\n",
        "# except ImportError:\n",
        "#   pass\n",
        "# else:\n",
        "#   files.download(\"embedding_vectors.tsv\")\n",
        "#   files.download(\"embedding_metadata.tsv\")"
      ],
      "metadata": {
        "id": "k_6Mx76TL1rT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks(RNN's)\n",
        "\n",
        "For further experimentations we're going to use special kind of neural networks used for sequence data such as to predict the next location based on the prior location or may be to generate a new sequence based on the past sequences which is done through **Recurrent Neural Networks (RNN)**.\n",
        "\n",
        "Recurrent Neural Networks can be used for a number of sequence-based problems:\n",
        "- **One to One:** one input, one output, such as image classification\n",
        "- **One to many:** one input, many output, such as image captioning\n",
        "- **Many to one:** many inputs, one outputs, such as text classification\n",
        "- **Many to Many:** many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text\n",
        "\n",
        "Most commong RNN cell or layers used for designing the network are:\n",
        "- LSTM (Long Short Term Memory)\n",
        "- GRU (Gates Recurrent Unit)\n",
        "- Bidirectional RNNs (passes forward and backward along a sequence, left to right and right to left)\n",
        "\n",
        "The architecture of the RNNs would be,\n",
        "\n",
        "      Input(text) -> Tokenize -> Embedding -> Layers -> Output (label probability)"
      ],
      "metadata": {
        "id": "DteZn62KNfRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 : LSTM\n",
        "\n",
        "> **Note**: For a best practice when we are comparing different models then embedding layer should be different because embedding layer is a learned representation of words, if we were to use the same embedding layer for each model, we'd be mixing what one model has learned with the next. "
      ],
      "metadata": {
        "id": "uEYEHgyjmVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed and creating embedding layer (new embedding layer for each model)\n",
        "tf.random.set_seed(17)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                           output_dim=128,\n",
        "                                           embeddings_initializer=\"uniform\",\n",
        "                                           input_length=max_length,\n",
        "                                           name=\"embedding_2\")\n",
        "\n",
        "# Create LSTM model\n",
        "input = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(input)\n",
        "x = model_2_embedding(x)\n",
        "#print(x.shape)\n",
        "#x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (if we want to stack RNN cells as long as return_sequences=True)\n",
        "x = layers.LSTM(64)(x)\n",
        "output = layers.Dense(1, activation='sigmoid')(x)\n",
        "model_2 = tf.keras.Model(input, output, name=\"model_2_LSTM\")"
      ],
      "metadata": {
        "id": "0SjUQmhdmdth"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the Tensorflow documentation on LSTM, it accepts 3D input tensor as [batch, timestamp, feature_vector] so when we stack one more cell of LSTM, then we must set `return_sequences=True` so that when next LSTM cell is stacked will be inject with 3D input or else it would through error of \"expecting 3D tensor but received 2D\""
      ],
      "metadata": {
        "id": "hPDrDQwSqlso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Function for Compiling and Fitting the Model"
      ],
      "metadata": {
        "id": "XWRxF6ChsAew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for compiling and fitting the model let's create a helper function\n",
        "\n",
        "def compile_fit_RNNs(model, dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Function used for compiling and fitting the model and save the \n",
        "  tensorboard experiment as well on the passed directory\n",
        "\n",
        "  Returns the model history\n",
        "  \"\"\"\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # fit the model\n",
        "  history = model.fit(train_sentences,\n",
        "                      train_labels,\n",
        "                      epochs=5,\n",
        "                      validation_data=(val_sentences, val_labels),\n",
        "                      callbacks=[create_tensorboard_callback(dir_name, experiment_name)])\n",
        "  \n",
        "  return history"
      ],
      "metadata": {
        "id": "t2esO0x8qZfU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_history = compile_fit_RNNs(model=model_2, dir_name=SAVE_DIR, experiment_name=\"model_2_LSTM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw7zOCCtr90b",
        "outputId": "b5adadf4-0424-4faf-b047-8f1964c656ac"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tensorboard log files to: model_logs/model_2_LSTM/20230207-045400\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 10s 14ms/step - loss: 0.5133 - accuracy: 0.7434 - val_loss: 0.4492 - val_accuracy: 0.7979\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3184 - accuracy: 0.8695 - val_loss: 0.4724 - val_accuracy: 0.7966\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.2199 - accuracy: 0.9204 - val_loss: 0.5474 - val_accuracy: 0.7874\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.1537 - accuracy: 0.9456 - val_loss: 0.6827 - val_accuracy: 0.7782\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.1108 - accuracy: 0.9603 - val_loss: 0.8267 - val_accuracy: 0.7664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we check the summary of the LSTM Model"
      ],
      "metadata": {
        "id": "ENi7wJatsq18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMvUY6bgsuW3",
        "outputId": "7fcc7fad-295c-475a-e03d-1e136eaa9651"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2_LSTM\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                49408     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,329,473\n",
            "Trainable params: 1,329,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make some predictions\n",
        "model_2_pred_probs = model_2.predict(val_sentences)\n",
        "model_2_pred_probs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsvQ90rasR-m",
        "outputId": "428dfed4-6aff-4d9f-87a5-22fea893af0a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02115481],\n",
              "       [0.00182996],\n",
              "       [0.01620476],\n",
              "       [0.16472118],\n",
              "       [0.9998977 ],\n",
              "       [0.99410546],\n",
              "       [0.01473757],\n",
              "       [0.7996621 ],\n",
              "       [0.8936984 ],\n",
              "       [0.54785395]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert them to compare with labels\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_2_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idYU2tb9sbcV",
        "outputId": "5a4269f3-ef74-420f-8dbd-46a82d726904"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 1., 0., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate the results\n",
        "model_2_results = calculate_results(y_true=val_labels,\n",
        "                                    y_pred=model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC0KzqZpsnJ6",
        "outputId": "acffd556-1986-4d6d-fd2a-ff3a46833349"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 76.64041994750657,\n",
              " 'precision': 0.7659025055163764,\n",
              " 'recall': 0.7664041994750657,\n",
              " 'f1-score': 0.7660201893659146}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the baseline results with model_2\n",
        "compare_baseline_to_new_results(baseline_results, model_2_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0QZl372tc4T",
        "outputId": "6273391e-f79f-4263-ffa9-25317f7d0817"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 80.18, New accuracy: 76.64, Difference: -3.543\n",
            "Baseline precision: 0.81, New precision: 0.77, Difference: -0.047\n",
            "Baseline recall: 0.80, New recall: 0.77, Difference: -0.035\n",
            "Baseline f1-score: 0.80, New f1-score: 0.77, Difference: -0.031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3 : GRU\n",
        "\n",
        "The GRU cell has similar features to an LSTM cell but has less parameters."
      ],
      "metadata": {
        "id": "Qx7XI2aKt-10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the random seed and create new embedding layer\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer='uniform',\n",
        "                                     input_length=max_length,\n",
        "                                     name=\"embedding_3\")\n",
        "\n",
        "# Build GRU model\n",
        "inputs = layers.Input(shape=(1,), dtype='string')\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_3_embedding(x)\n",
        "x = layers.GRU(64)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "model_3 = tf.keras.Model(inputs, outputs, name='model_3_GRU')\n"
      ],
      "metadata": {
        "id": "4tnDsQXHuIgS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see the summary\n",
        "model_3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NyQL49vvR7y",
        "outputId": "2deb5a4f-fa5c-4f57-e197-27ea06e8163d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3_GRU\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 64)                37248     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,317,313\n",
            "Trainable params: 1,317,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's the same as LSTM model but with less params"
      ],
      "metadata": {
        "id": "DfdhvEYwveuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and fit the model\n",
        "model_3_history = compile_fit_RNNs(model_3, SAVE_DIR, \"model_3_GRU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5dlJVK3vlXw",
        "outputId": "4d0ec5e4-8963-4309-d98d-a9f6948078f5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tensorboard log files to: model_logs/model_3_GRU/20230207-045426\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 5s 11ms/step - loss: 0.5328 - accuracy: 0.7219 - val_loss: 0.4518 - val_accuracy: 0.8045\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 8ms/step - loss: 0.3221 - accuracy: 0.8641 - val_loss: 0.4690 - val_accuracy: 0.8018\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.2178 - accuracy: 0.9181 - val_loss: 0.5357 - val_accuracy: 0.7848\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.1489 - accuracy: 0.9499 - val_loss: 0.6772 - val_accuracy: 0.7703\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 8ms/step - loss: 0.1122 - accuracy: 0.9625 - val_loss: 0.6983 - val_accuracy: 0.7559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "# convert them into labels\n",
        "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs)) \n",
        "model_3_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax7jpoHjvwRR",
        "outputId": "1e81c127-6240-46bc-ee45-46536e1a20e3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 1., 0., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the result\n",
        "model_3_results = calculate_results(y_true=val_labels,\n",
        "                                   y_pred=model_3_preds)\n",
        "model_3_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9FOAOdQwE2b",
        "outputId": "522d5923-86b5-4c67-feef-13e7d467a3de"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 75.59055118110236,\n",
              " 'precision': 0.7551876442680979,\n",
              " 'recall': 0.7559055118110236,\n",
              " 'f1-score': 0.7551196015270819}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with baseline score\n",
        "compare_baseline_to_new_results(baseline_results, model_3_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuuFbg8-wOo1",
        "outputId": "ef5a1d38-9968-4030-8291-26f0c0fb8c10"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 80.18, New accuracy: 75.59, Difference: -4.593\n",
            "Baseline precision: 0.81, New precision: 0.76, Difference: -0.057\n",
            "Baseline recall: 0.80, New recall: 0.76, Difference: -0.046\n",
            "Baseline f1-score: 0.80, New f1-score: 0.76, Difference: -0.042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's looks like Baseline is still outperforming dense models, let's see one more model"
      ],
      "metadata": {
        "id": "_yuiOQbYwXjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4 : Bidirectional RNNs\n",
        "\n",
        "A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left."
      ],
      "metadata": {
        "id": "_VJlzkRkwhIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(17)\n",
        "model_4_embedding = layers.Embedding(input_dim = max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer='uniform',\n",
        "                                     input_length=max_length,\n",
        "                                     name=\"embedding_4\")\n",
        "\n",
        "# Build the Bidirectional Model\n",
        "inputs = layers.Input(shape=(1,), dtype='string')\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_4_embedding(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "model_4 = tf.keras.Model(inputs, outputs, name='model_4_Bidirectional')"
      ],
      "metadata": {
        "id": "uqprPWR_wlLZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the summary\n",
        "model_4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wHIKZp3xitb",
        "outputId": "625702c6-9de3-4c8c-8300-d43b1f261df1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4_Bidirectional\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              98816     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,378,945\n",
            "Trainable params: 1,378,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the parameters of the Bidirectional layer get doubled."
      ],
      "metadata": {
        "id": "hEAbh_vcxnqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and fit\n",
        "model_4_history = compile_fit_RNNs(model_4, SAVE_DIR, \"model_4_Bidirectional\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxIMSyXOxtKd",
        "outputId": "2bc824eb-27e1-42e5-f007-976a5271f254"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tensorboard log files to: model_logs/model_4_Bidirectional/20230207-045450\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 16ms/step - loss: 0.5140 - accuracy: 0.7427 - val_loss: 0.4542 - val_accuracy: 0.8018\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.3149 - accuracy: 0.8691 - val_loss: 0.4835 - val_accuracy: 0.7900\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.2171 - accuracy: 0.9202 - val_loss: 0.5680 - val_accuracy: 0.7795\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.1469 - accuracy: 0.9520 - val_loss: 0.6563 - val_accuracy: 0.7612\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.1036 - accuracy: 0.9641 - val_loss: 0.7833 - val_accuracy: 0.7598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "model_4_pred_probs = model_4.predict(val_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js6QrUtEx1t8",
        "outputId": "2c4b1696-b355-4db8-dca6-adc5a3d66d61"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert them into labels\n",
        "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uo1RB9Lx5yk",
        "outputId": "04969043-47a2-4935-92de-0f7f449839e4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 1., 0., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the result\n",
        "model_4_results = calculate_results(y_true = val_labels,\n",
        "                                    y_pred = model_4_preds)\n",
        "model_4_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXe4FqILyEfc",
        "outputId": "6bf8cadd-8865-4e3d-bafd-85cbfde0e2c5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 75.98425196850394,\n",
              " 'precision': 0.7600694650010854,\n",
              " 'recall': 0.7598425196850394,\n",
              " 'f1-score': 0.7599441744417845}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with baseline scores\n",
        "compare_baseline_to_new_results(baseline_results, model_4_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UJ2lQftyQvP",
        "outputId": "26af6601-7508-4bdb-df92-0d76bc09d7cf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 80.18, New accuracy: 75.98, Difference: -4.199\n",
            "Baseline precision: 0.81, New precision: 0.76, Difference: -0.052\n",
            "Baseline recall: 0.80, New recall: 0.76, Difference: -0.042\n",
            "Baseline f1-score: 0.80, New f1-score: 0.76, Difference: -0.037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution Neural Networks for Text\n",
        "\n",
        "CNNs can be used to train Text inputs by using the 1-Dimension layers instead of 2-Dimensional convolution like did with Images.\n",
        "\n",
        "A Typical CNN architecture for sequences will look like the following:\n",
        "\n",
        "```\n",
        "Inputs (text) -> Tokenization -> Embedding -> Layers -> Outputs (class probabilities)\n",
        "```\n",
        "\n",
        "The main difference would be to use `tf.keras.layers.Conv1D()` instead of LSTM, GRU or Bidirectional cell in the layers, rest will be same."
      ],
      "metadata": {
        "id": "lyGbW4SxjYpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5 : Conv1D \n",
        "\n",
        "Let's see first 1-dimensional convolutional layer (also called a temporal convolution), temporal means the data in timestamp sequences."
      ],
      "metadata": {
        "id": "HBf2K7hbj0sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our embedding, 1D convolutional and max pooling\n",
        "embedding_test = embedding(text_vectorizer(['This is a test for conv layer'])) \n",
        "conv_1d = layers.Conv1D(filters=32, kernel_size=5, strides=1, activation=\"relu\", padding=\"valid\")\n",
        "conv_1d_output = conv_1d(embedding_test)\n",
        "max_pool = layers.GlobalMaxPool1D()\n",
        "max_pool_output = max_pool(conv_1d_output)\n",
        "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyN-Ks1bkVHI",
        "outputId": "abbc0a4e-c4af-464c-abd5-2b809bc6a5a0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shape of `conv_1d_output` is `[1,11,32]`, here the input length was `15` and output_dim - `128` of sequences in the embedding layer.\n",
        "\n",
        "The 1-D conv layer compressed inline with its parameters. And the same goes for max pooling layer output.\n",
        "\n",
        "Our text starts out as a string but gets converted to a feature vector of length 32 through various transformation steps(from tokenization to embedding to 1-Dimensional convolution to max pool)."
      ],
      "metadata": {
        "id": "l-nT5bXOk63W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## see the outputs of each layer\n",
        "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu8uiayyn79s",
        "outputId": "5387ea00-21c8-45e9-c192-92f05d823def"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              " array([[[ 0.00563604,  0.00831303, -0.03987604, ..., -0.00901501,\n",
              "          -0.0245837 , -0.00313253],\n",
              "         [ 0.01050906,  0.07342832, -0.02361131, ..., -0.01137419,\n",
              "           0.03161565,  0.01970428],\n",
              "         [-0.03601194,  0.00866882, -0.04841171, ..., -0.04186255,\n",
              "           0.03951385, -0.02277594],\n",
              "         ...,\n",
              "         [-0.05793181, -0.03914463, -0.02473384, ...,  0.00409453,\n",
              "           0.0232612 , -0.00015125],\n",
              "         [-0.05793181, -0.03914463, -0.02473384, ...,  0.00409453,\n",
              "           0.0232612 , -0.00015125],\n",
              "         [-0.05793181, -0.03914463, -0.02473384, ...,  0.00409453,\n",
              "           0.0232612 , -0.00015125]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n",
              " array([[[0.        , 0.03461807, 0.04056319, 0.03943498, 0.03135174,\n",
              "          0.00109526, 0.00164499, 0.02065375, 0.03384815, 0.06376338,\n",
              "          0.        , 0.        , 0.        , 0.        , 0.02814317,\n",
              "          0.        , 0.02335803, 0.        , 0.        , 0.08719777,\n",
              "          0.09039338, 0.        , 0.03038756, 0.        , 0.        ,\n",
              "          0.        , 0.01343111, 0.        , 0.06609245, 0.        ,\n",
              "          0.        , 0.        ],\n",
              "         [0.0764782 , 0.        , 0.01919918, 0.        , 0.03547236,\n",
              "          0.08662464, 0.        , 0.        , 0.        , 0.08428136,\n",
              "          0.05702461, 0.02060775, 0.        , 0.        , 0.        ,\n",
              "          0.0259808 , 0.        , 0.        , 0.01701439, 0.        ,\n",
              "          0.        , 0.08233301, 0.        , 0.        , 0.        ,\n",
              "          0.        , 0.00827227, 0.04662843, 0.02168218, 0.04228193,\n",
              "          0.0817068 , 0.00100022],\n",
              "         [0.        , 0.05717973, 0.        , 0.        , 0.        ,\n",
              "          0.0749508 , 0.        , 0.0204897 , 0.        , 0.05699942,\n",
              "          0.1102318 , 0.05702512, 0.        , 0.02310777, 0.        ,\n",
              "          0.        , 0.00686768, 0.        , 0.00701855, 0.08093305,\n",
              "          0.10979289, 0.1302534 , 0.        , 0.03066099, 0.        ,\n",
              "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.        , 0.01952722],\n",
              "         [0.02640982, 0.01735174, 0.01103687, 0.08510982, 0.01400458,\n",
              "          0.05367159, 0.04072631, 0.01060584, 0.        , 0.06676361,\n",
              "          0.        , 0.08392848, 0.04772657, 0.        , 0.        ,\n",
              "          0.        , 0.        , 0.03650603, 0.        , 0.03884183,\n",
              "          0.        , 0.06922229, 0.0986066 , 0.        , 0.        ,\n",
              "          0.06090887, 0.        , 0.00417486, 0.02106519, 0.09397036,\n",
              "          0.06676272, 0.        ],\n",
              "         [0.01558588, 0.        , 0.02263872, 0.00055924, 0.00456196,\n",
              "          0.0150536 , 0.03777018, 0.        , 0.        , 0.05350943,\n",
              "          0.01694094, 0.00364913, 0.0145974 , 0.        , 0.        ,\n",
              "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "          0.0098661 , 0.06423342, 0.07255761, 0.        , 0.        ,\n",
              "          0.        , 0.04436693, 0.        , 0.02071763, 0.04013617,\n",
              "          0.05641007, 0.        ],\n",
              "         [0.        , 0.03461329, 0.0522542 , 0.01082174, 0.02114743,\n",
              "          0.        , 0.        , 0.        , 0.        , 0.03142166,\n",
              "          0.0248225 , 0.        , 0.02933151, 0.01765052, 0.        ,\n",
              "          0.        , 0.01048655, 0.05598029, 0.        , 0.        ,\n",
              "          0.        , 0.02534768, 0.0564429 , 0.        , 0.        ,\n",
              "          0.        , 0.07428715, 0.        , 0.        , 0.        ,\n",
              "          0.00819662, 0.        ],\n",
              "         [0.        , 0.07465488, 0.        , 0.        , 0.03011666,\n",
              "          0.        , 0.        , 0.        , 0.        , 0.08046453,\n",
              "          0.0639257 , 0.0381645 , 0.05070102, 0.        , 0.        ,\n",
              "          0.        , 0.02316467, 0.0295087 , 0.        , 0.00791348,\n",
              "          0.        , 0.00830845, 0.0534264 , 0.03598184, 0.        ,\n",
              "          0.        , 0.06427152, 0.        , 0.        , 0.        ,\n",
              "          0.01793887, 0.        ],\n",
              "         [0.        , 0.06735586, 0.        , 0.        , 0.04449612,\n",
              "          0.00111308, 0.        , 0.        , 0.        , 0.06157278,\n",
              "          0.11221662, 0.01950593, 0.03782241, 0.00625204, 0.        ,\n",
              "          0.        , 0.02321805, 0.        , 0.        , 0.        ,\n",
              "          0.01971479, 0.0091337 , 0.02561825, 0.0506812 , 0.0062118 ,\n",
              "          0.        , 0.08538773, 0.        , 0.        , 0.        ,\n",
              "          0.04578616, 0.        ],\n",
              "         [0.        , 0.06735587, 0.        , 0.        , 0.04449612,\n",
              "          0.00111307, 0.        , 0.        , 0.        , 0.06157279,\n",
              "          0.11221663, 0.01950593, 0.03782241, 0.00625204, 0.        ,\n",
              "          0.        , 0.02321806, 0.        , 0.        , 0.        ,\n",
              "          0.01971478, 0.0091337 , 0.02561824, 0.05068121, 0.0062118 ,\n",
              "          0.        , 0.08538774, 0.        , 0.        , 0.        ,\n",
              "          0.04578616, 0.        ],\n",
              "         [0.        , 0.06735587, 0.        , 0.        , 0.04449611,\n",
              "          0.00111307, 0.        , 0.        , 0.        , 0.06157279,\n",
              "          0.11221663, 0.01950593, 0.0378224 , 0.00625204, 0.        ,\n",
              "          0.        , 0.02321807, 0.        , 0.        , 0.        ,\n",
              "          0.01971477, 0.0091337 , 0.02561824, 0.0506812 , 0.00621179,\n",
              "          0.        , 0.08538773, 0.        , 0.        , 0.        ,\n",
              "          0.04578616, 0.        ],\n",
              "         [0.        , 0.06735586, 0.        , 0.        , 0.04449612,\n",
              "          0.00111307, 0.        , 0.        , 0.        , 0.06157279,\n",
              "          0.11221662, 0.01950593, 0.03782241, 0.00625204, 0.        ,\n",
              "          0.        , 0.02321807, 0.        , 0.        , 0.        ,\n",
              "          0.01971478, 0.00913371, 0.02561823, 0.05068121, 0.0062118 ,\n",
              "          0.        , 0.08538773, 0.        , 0.        , 0.        ,\n",
              "          0.04578616, 0.        ]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n",
              " array([[0.0764782 , 0.07465488, 0.0522542 , 0.08510982, 0.04449612,\n",
              "         0.08662464, 0.04072631, 0.02065375, 0.03384815, 0.08428136,\n",
              "         0.11221663, 0.08392848, 0.05070102, 0.02310777, 0.02814317,\n",
              "         0.0259808 , 0.02335803, 0.05598029, 0.01701439, 0.08719777,\n",
              "         0.10979289, 0.1302534 , 0.0986066 , 0.05068121, 0.0062118 ,\n",
              "         0.06090887, 0.08538774, 0.04662843, 0.06609245, 0.09397036,\n",
              "         0.0817068 , 0.01952722]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the model"
      ],
      "metadata": {
        "id": "IeqGZ-isoDro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(17)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer='uniform',\n",
        "                                     input_length=max_length,\n",
        "                                     name='embedding_5')\n",
        "\n",
        "# Create 1-D conv layer\n",
        "inputs = layers.Input(shape=(1,), dtype='string')\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_5_embedding(x)\n",
        "x = layers.Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "model_5 = tf.keras.Model(inputs, outputs, name='model_5_Conv1D')\n",
        "\n",
        "# see the summary of the model\n",
        "model_5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI9CIi6AoKca",
        "outputId": "022c6655-5c7b-46ca-8a1c-65a3a4a91835"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5_Conv1D\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_1 (TextV  (None, 15)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, 15, 128)           1280000   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 11, 64)            41024     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,321,089\n",
            "Trainable params: 1,321,089\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and fit the model\n",
        "model_5_history = compile_fit_RNNs(model_5, SAVE_DIR, \"Conv1D_model_5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxAmZLe8pPmy",
        "outputId": "80859da8-826b-4a24-faa5-745a4da4e534"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tensorboard log files to: model_logs/Conv1D_model_5/20230207-045511\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 4s 8ms/step - loss: 0.5492 - accuracy: 0.7230 - val_loss: 0.4671 - val_accuracy: 0.7979\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 2s 8ms/step - loss: 0.3318 - accuracy: 0.8626 - val_loss: 0.4970 - val_accuracy: 0.7808\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 2s 8ms/step - loss: 0.1982 - accuracy: 0.9313 - val_loss: 0.5820 - val_accuracy: 0.7808\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 1s 6ms/step - loss: 0.1228 - accuracy: 0.9578 - val_loss: 0.7034 - val_accuracy: 0.7703\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 2s 8ms/step - loss: 0.0885 - accuracy: 0.9712 - val_loss: 0.7730 - val_accuracy: 0.7651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make some predictions\n",
        "model_5_pred_probs = model_5.predict(val_sentences)\n",
        "# convert them into labels\n",
        "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
        "\n",
        "model_5_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6jw6OhNpndy",
        "outputId": "0ba98efd-32f5-486e-9d30-4f8f393f6506"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 1., 0., 1., 1., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the results\n",
        "model_5_results = calculate_results(y_true=val_labels,\n",
        "                                    y_pred=model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDaJpYHpp6rU",
        "outputId": "5c18b70d-fade-496e-f709-54e5b7fc02d5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 76.50918635170603,\n",
              " 'precision': 0.7644431182527525,\n",
              " 'recall': 0.7650918635170604,\n",
              " 'f1-score': 0.7642853951017926}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_5_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5BqqHSjqHUW",
        "outputId": "bd530562-c37d-44ca-e0b1-d2a7fc1e5af7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 80.18, New accuracy: 76.51, Difference: -3.675\n",
            "Baseline precision: 0.81, New precision: 0.76, Difference: -0.048\n",
            "Baseline recall: 0.80, New recall: 0.77, Difference: -0.037\n",
            "Baseline f1-score: 0.80, New f1-score: 0.76, Difference: -0.033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Pretrained Embeddings (transfer learning for NLP)\n",
        "\n",
        "A common practice is to leverage pretrained embeddings through transfer learning. This is one of the main benefits of using deep models: being able to take what one (often larger) model has learned (often on a large amount of data) and adjust it for our own use case.\n",
        "\n",
        "Instead of using our own embedding layer, we're going to replace with a pretrained embedding layer.\n",
        "\n",
        "We're going to use the [Universal Sentence Encoder](https://www.aclweb.org/anthology/D18-2029.pdf) from Tensorflow Hub (a great resource containing a plethora of pretrained model resources for a variety of tasks.)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ie65o3e8OOmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 6 : TF Hub Pretrained Sentence Encoder\n",
        "\n",
        "Universal Sentence Encoder creates whole sentence-level embedding rather than creating a word-level embedding that we did earlier.\n",
        "\n",
        "Our embedding layer outputs an 128 dimensional vector for each word, where as, the Universal Sentence Encoder outputs a 512 dimensional vector for each sentence."
      ],
      "metadata": {
        "id": "LNXkaZg8O8IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2XyCmPyLPq7V",
        "outputId": "ddc67739-469c-472e-b1f0-dafa08ec1cea"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"There's a flood in my village!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load universal sentence encoder\n",
        "embed_samples = embed([sample_sentence, \n",
        "                       \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
        "\n",
        "print(embed_samples[0][:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfIRbZUTPeQX",
        "outputId": "6de93e0c-21d2-4523-b3fe-94050ec668fc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[-0.00875852  0.02968347  0.03308665 -0.02835228 -0.01226963  0.08441445\n",
            "  0.03303674  0.05103541 -0.01697742 -0.01577765  0.06972899  0.00490609\n",
            " -0.02069244  0.07354212  0.07200169 -0.02784333  0.00856447 -0.05491454\n",
            "  0.02859962 -0.03333077 -0.01728002  0.0557287   0.04092591  0.05927492\n",
            " -0.01725632 -0.04377813 -0.00842895 -0.00564028 -0.04898241 -0.02710576\n",
            " -0.03276608  0.0313632  -0.00436723 -0.03436001  0.03699801 -0.04601654\n",
            "  0.04075051  0.03726257 -0.03553419 -0.0628173  -0.03123069 -0.0353759\n",
            "  0.00546736  0.05002001 -0.09777351 -0.06816328 -0.01440394 -0.01097005\n",
            " -0.04983862  0.0248761 ], shape=(50,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each sentence has been encoded into a 512 dimension vector\n",
        "embed_samples[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1RDJOQjP2N3",
        "outputId": "ea86494d-66ce-47ad-b1bd-70665590f6ae"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([512])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use the USE module into a Keras layer using the `hub.KerasLayer` class"
      ],
      "metadata": {
        "id": "RsY2p6boP8bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_encoder_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
        "                                        input_shape=[], # shape of inputs coming to our model\n",
        "                                        dtype=tf.string,\n",
        "                                        trainable=False,\n",
        "                                        name=\"USE\")"
      ],
      "metadata": {
        "id": "gH0SlnkNQGrv"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model using Sequential API\n",
        "model_6 = tf.keras.Sequential([\n",
        "    sentence_encoder_layer,\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid', name='output_layer')\n",
        "], name='model_6_USE')\n",
        "\n",
        "# compile and fit the model\n",
        "model_6_history = compile_fit_RNNs(model_6, SAVE_DIR, \"tf_hub_sentence_encoder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "373NCa0_QYKi",
        "outputId": "ca277cee-abaa-4e32-b0af-545c8517579c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Tensorboard log files to: model_logs/tf_hub_sentence_encoder/20230207-050526\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 10s 31ms/step - loss: 0.5095 - accuracy: 0.7811 - val_loss: 0.4392 - val_accuracy: 0.8058\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.4143 - accuracy: 0.8175 - val_loss: 0.4306 - val_accuracy: 0.8189\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.3998 - accuracy: 0.8232 - val_loss: 0.4296 - val_accuracy: 0.8176\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.3906 - accuracy: 0.8278 - val_loss: 0.4292 - val_accuracy: 0.8202\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.3826 - accuracy: 0.8337 - val_loss: 0.4294 - val_accuracy: 0.8268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summary\n",
        "model_6.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zAJvB1QQ67H",
        "outputId": "ffd32201-fee8-44b7-ca13-e16b6792bebc"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6_USE\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " USE (KerasLayer)            (None, 512)               256797824 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                32832     \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 256,830,721\n",
            "Trainable params: 32,897\n",
            "Non-trainable params: 256,797,824\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions\n",
        "model_6_pred_probs = model_6.predict(val_sentences)\n",
        "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
        "\n",
        "model_6_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWJkD5ALQ87v",
        "outputId": "47757e77-a814-4d4f-c599-5b7f48d6d922"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 1s 14ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 1., 0., 1., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# results\n",
        "model_6_results = calculate_results(y_true=val_labels,\n",
        "                                    y_pred=model_6_preds)\n",
        "model_6_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeX3ae1BRN3v",
        "outputId": "2a87476b-1ba5-49e6-e5a2-c8577ac501d8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 82.67716535433071,\n",
              " 'precision': 0.8300866466031345,\n",
              " 'recall': 0.8267716535433071,\n",
              " 'f1-score': 0.8247090008454357}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_6_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEQhvKYnRbKh",
        "outputId": "08734ba9-aeeb-4478-94d3-9d797bb6525d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 80.18, New accuracy: 82.68, Difference: 2.493\n",
            "Baseline precision: 0.81, New precision: 0.83, Difference: 0.018\n",
            "Baseline recall: 0.80, New recall: 0.83, Difference: 0.025\n",
            "Baseline f1-score: 0.80, New f1-score: 0.82, Difference: 0.028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we are able to beat the baseline score but using pretrained embeddings"
      ],
      "metadata": {
        "id": "NosNIMgDRg_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 7 : TensorFlow Hub Pretrained Sentence Encoder 10% of the training data\n",
        "\n",
        "One of the main benefits of using transfer learning methods, such as, the pretrained embeddings within the USE is the ability to get great results on a small amount of data (the USE paper even mentions this in the abstract)."
      ],
      "metadata": {
        "id": "SP5j_xsPRnjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_shuffled"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qakknm_bUpo8",
        "outputId": "616a5672-3c38-417d-8475-ab8a40464d78"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id            keyword                    location  \\\n",
              "7027  10072            typhoon                         NaN   \n",
              "318     463         armageddon                         NaN   \n",
              "1681   2425            collide  www.youtube.com?Malkavius2   \n",
              "5131   7318  nuclear%20reactor          New York, New York   \n",
              "2967   4262           drowning          Hendersonville, NC   \n",
              "...     ...                ...                         ...   \n",
              "406     584              arson           Jerusalem, Israel   \n",
              "5510   7863        quarantined                 Livonia, MI   \n",
              "2191   3139             debris                         NaN   \n",
              "7409  10600            wounded               santo domingo   \n",
              "2671   3833           detonate    back in japan ??????????   \n",
              "\n",
              "                                                   text  target  \n",
              "7027  Typhoon Soudelor: When will it hit Taiwan ÛÒ ...       1  \n",
              "318   RT @RTRRTcoach: #Love #TrueLove #romance lith ...       0  \n",
              "1681  I liked a @YouTube video from @gassymexican ht...       0  \n",
              "5131  Japan's Restart of Nuclear Reactor Fleet Fast ...       1  \n",
              "2967  #ICYMI #Annoucement from Al Jackson... http://...       0  \n",
              "...                                                 ...     ...  \n",
              "406   Mourning notices for stabbing arson victims st...       1  \n",
              "5510  Reddit's new content policy goes into effect m...       0  \n",
              "2191  Plane debris discovered on Reunion Island belo...       1  \n",
              "7409  Police Officer Wounded Suspect Dead After Exch...       1  \n",
              "2671  Detonate (feat. M?.?O?.?P?.?)\\nfrom Grandeur b...       0  \n",
              "\n",
              "[7613 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8281a1e6-9de3-4c50-a898-905f3856bac7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7027</th>\n",
              "      <td>10072</td>\n",
              "      <td>typhoon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor: When will it hit Taiwan ÛÒ ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>463</td>\n",
              "      <td>armageddon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RT @RTRRTcoach: #Love #TrueLove #romance lith ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1681</th>\n",
              "      <td>2425</td>\n",
              "      <td>collide</td>\n",
              "      <td>www.youtube.com?Malkavius2</td>\n",
              "      <td>I liked a @YouTube video from @gassymexican ht...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5131</th>\n",
              "      <td>7318</td>\n",
              "      <td>nuclear%20reactor</td>\n",
              "      <td>New York, New York</td>\n",
              "      <td>Japan's Restart of Nuclear Reactor Fleet Fast ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2967</th>\n",
              "      <td>4262</td>\n",
              "      <td>drowning</td>\n",
              "      <td>Hendersonville, NC</td>\n",
              "      <td>#ICYMI #Annoucement from Al Jackson... http://...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>584</td>\n",
              "      <td>arson</td>\n",
              "      <td>Jerusalem, Israel</td>\n",
              "      <td>Mourning notices for stabbing arson victims st...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5510</th>\n",
              "      <td>7863</td>\n",
              "      <td>quarantined</td>\n",
              "      <td>Livonia, MI</td>\n",
              "      <td>Reddit's new content policy goes into effect m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2191</th>\n",
              "      <td>3139</td>\n",
              "      <td>debris</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Plane debris discovered on Reunion Island belo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7409</th>\n",
              "      <td>10600</td>\n",
              "      <td>wounded</td>\n",
              "      <td>santo domingo</td>\n",
              "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2671</th>\n",
              "      <td>3833</td>\n",
              "      <td>detonate</td>\n",
              "      <td>back in japan ??????????</td>\n",
              "      <td>Detonate (feat. M?.?O?.?P?.?)\\nfrom Grandeur b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8281a1e6-9de3-4c50-a898-905f3856bac7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8281a1e6-9de3-4c50-a898-905f3856bac7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8281a1e6-9de3-4c50-a898-905f3856bac7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "train_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),\n",
        "                                                                                                                            train_labels,\n",
        "                                                                                                                            test_size=0.1,\n",
        "                                                                                                                            random_state=17)"
      ],
      "metadata": {
        "id": "IMxJ7Lp1UX5E"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences_10_percent.shape, train_labels_10_percent.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrWNUwTeU-Qd",
        "outputId": "ab135ef7-0da5-4f43-8916-c3841962d57d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((686,), (686,))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the length of 10 percent datasets\n",
        "print(f'Total Training examples: {len(train_sentences)}')\n",
        "print(f'Length of 10% training examples: {len(train_sentences_10_percent)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd-rtL16VJWA",
        "outputId": "a4dbc505-7509-411f-a0dc-c18deb5047bd"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training examples: 6851\n",
            "Length of 10% training examples: 686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the number of targets in our subset of data\n",
        "pd.Series(train_labels_10_percent).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN7fqnIBVc8l",
        "outputId": "d89d23dc-8abf-4726-f789-2526c5508646"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    409\n",
              "1    277\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to use the same architecture of `model_6` then we can try using cloning the model using `tf.keras.models.clone_model()`, doing this will create the same architecture but reset the learned weights of the clone target (pretrained weights from the USE will remain but all other will be reset)"
      ],
      "metadata": {
        "id": "l8ZKt6uFVzcr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSBN9vYBWhPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}